{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bbcc7b7-63f1-4893-934c-9862cf2f2b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 2.0.5 (you have 1.4.7). Upgrade using: pip install --upgrade albumentations\n",
      "/home/neos/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:datasets:PyTorch version 2.6.0+cu124 available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, AutoImageProcessor\n",
    "from datasets import load_dataset\n",
    "\n",
    "rescale = 4\n",
    "PATCH_SIZE = 256\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "CUSTOM_MEAN = [0.1778, 0.2696, 0.1686]\n",
    "CUSTOM_STD = [0.0942, 0.0915, 0.0762]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65adc14f-d9d7-442f-8efb-2f322a4dfdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 8658 examples [00:01, 7152.40 examples/s]\n",
      "Generating validation split: 918 examples [00:00, 8015.26 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"my_superres_dataset.py\", # путь к файлу с классом датасета\n",
    "    data_dir=\"./data_overlapped\", # корень, где лежат папки data/ и data_x4/\n",
    "    # split=[\"train\", \"validation\"], # какие сплиты грузить\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "train_ds = dataset[\"train\"]\n",
    "valid_ds = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83b88fa8-e2a1-49c9-9dd9-5799bb7a1dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['hr_image', 'lr_image', 'filename'],\n",
       "    num_rows: 8658\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a9bf592-3180-4301-baa9-2a4bac73bbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Функция для вычисления метрик на валидации (PSNR, MSE и пр.).\n",
    "    eval_pred – это (logits, labels),\n",
    "    где logits и labels – тензоры [batch_size, C, H, W].\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    mse = ((logits - labels) ** 2).mean()\n",
    "    psnr = 10 * math.log10(1 / mse.item()) # при предположении нормализации [0,1]\n",
    "    return {\n",
    "    \"mse\": mse.item(),\n",
    "    \"psnr\": psnr,\n",
    "    }\n",
    "\n",
    "# def data_collator(features):\n",
    "#     \"\"\"\n",
    "#     Функция для формирования батча.\n",
    "#     Приводит входные данные к нужному формату для Trainer.\n",
    "#     \"\"\"\n",
    "#     lr_batch = []\n",
    "#     hr_batch = []\n",
    "#     for f in features:\n",
    "#         lr_batch.append(f[\"lr\"])\n",
    "#         hr_batch.append(f[\"hr\"])\n",
    "\n",
    "#     lr_batch = torch.stack(lr_batch)\n",
    "#     hr_batch = torch.stack(hr_batch)\n",
    "    \n",
    "#     return {\n",
    "#         \"pixel_values\": lr_batch.float(),  # входные данные (x_LR)\n",
    "#         \"labels\": hr_batch.float(),        # метка (x_HR)\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e76a7bd3-6266-4eac-860d-8281ab1d3423",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "def transforms(examples):\n",
    "    transformA = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        \n",
    "        # При необходимости можно добавить и другие аугментации, \n",
    "        # например, яркость/контраст, шум и т.д.\n",
    "        \n",
    "        # Простейшая нормализация, если нужно\n",
    "        # A.Normalize(mean=CUSTOM_MEAN, std=CUSTOM_STD),  \n",
    "        ToTensorV2()\n",
    "    ],\n",
    "    additional_targets={\n",
    "    \"lr_image\": \"image\"  # Говорим, что \"lr_image\" — это тоже \"image\"\n",
    "    },\n",
    "    is_check_shapes=False,\n",
    "    )\n",
    "    transformed_images, transformed_labels = [], []\n",
    "    for image, lr_image in zip(examples[\"hr_image\"], examples[\"lr_image\"]):\n",
    "        image, lr_image = np.array(image), np.array(lr_image)\n",
    "        transformed = transformA(image=image, lr_image=lr_image)\n",
    "        transformed_labels.append(transformed[\"image\"])\n",
    "        transformed_images.append(transformed[\"lr_image\"])\n",
    "    examples_res = dict()\n",
    "    examples_res[\"pixel_values\"] = transformed_images\n",
    "    examples_res[\"labels\"] = transformed_labels\n",
    "    return examples_res\n",
    "\n",
    "def transforms_val(examples):\n",
    "    transformA = A.Compose([\n",
    "        # A.HorizontalFlip(p=0.5),\n",
    "        # A.VerticalFlip(p=0.5),\n",
    "        # A.RandomRotate90(p=0.5),\n",
    "        \n",
    "        # При необходимости можно добавить и другие аугментации, \n",
    "        # например, яркость/контраст, шум и т.д.\n",
    "        \n",
    "        # Простейшая нормализация, если нужно\n",
    "        # A.Normalize(mean=CUSTOM_MEAN, std=CUSTOM_STD),  \n",
    "        A.Normalize(mean=[0., 0., 0.], std=[1., 1., 1.]), \n",
    "        ToTensorV2()\n",
    "    ],\n",
    "    additional_targets={\n",
    "    \"lr_image\": \"image\"  # Говорим, что \"lr_image\" — это тоже \"image\"\n",
    "    },\n",
    "    is_check_shapes=False,\n",
    "    )\n",
    "    transformed_images, transformed_labels = [], []\n",
    "    for image, lr_image in zip(examples[\"hr_image\"], examples[\"lr_image\"]):\n",
    "        image, lr_image = np.array(image), np.array(lr_image)\n",
    "        transformed = transformA(image=image, lr_image=lr_image)\n",
    "        transformed_labels.append(transformed[\"image\"])\n",
    "        transformed_images.append(transformed[\"lr_image\"])\n",
    "    examples_res = dict()\n",
    "    examples_res[\"pixel_values\"] = transformed_images\n",
    "    examples_res[\"labels\"] = transformed_labels\n",
    "    return examples_res\n",
    "\n",
    "from torchvision.transforms import ColorJitter\n",
    "\n",
    "jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"caidas/swin2SR-classical-sr-x4-64\", image_mean=CUSTOM_MEAN, image_std=CUSTOM_STD)\n",
    "\n",
    "def train_transforms(example_batch):\n",
    "    example_batch = transforms(example_batch)\n",
    "    # images = [jitter(x) for x in example_batch[\"lr_image\"]]\n",
    "    images = [x for x in example_batch[\"lr_image\"]]\n",
    "    labels = [x for x in example_batch[\"hr_image\"]]\n",
    "    # inputs = image_processor([images, labels], return_tensors=\"pt\")\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def val_transforms(example_batch):\n",
    "    example_batch = transforms_val(example_batch)\n",
    "    # images = [x for x in example_batch[\"lr_image\"]]\n",
    "    # labels = [x for x in example_batch[\"hr_image\"]]\n",
    "    # print(len(images))\n",
    "    # print(example_batch[\"hr_image\"])\n",
    "    # inputs = image_processor(images + labels, return_tensors=\"pt\")\n",
    "    # example_batch.pop(\"labels\")\n",
    "    return example_batch\n",
    "\n",
    "\n",
    "train_ds.set_transform(train_transforms)\n",
    "valid_ds.set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70b7e338-3085-4be9-b944-d2d07581a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sr.TTST.model_archs.TTST_arc import TTST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "840745ae-1444-4c10-a9af-2a2a2ae333ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import ImageSuperResolutionOutput\n",
    "from typing import Optional, Union, Tuple\n",
    "\n",
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model, patch_size=PATCH_SIZE, accepts_loss_kwargs=False):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.accepts_loss_kwargs = accepts_loss_kwargs\n",
    "        self.patch_size = patch_size\n",
    "        self.loss = torch.nn.L1Loss()\n",
    "\n",
    "    # def forward(self, *args, **kwargs):\n",
    "    #     outputs = self.main(*args, **kwargs)\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: torch.FloatTensor,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, ImageSuperResolutionOutput]:\n",
    "        outputs = self.model(pixel_values)\n",
    "        loss, logits_val = self.loss(outputs, labels), outputs\n",
    "        return ImageSuperResolutionOutput(loss=loss, reconstruction=logits_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcfd46d-e584-4020-b233-5a2774ef7c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
    "os.environ[\"WANDB_WATCH\"] = \"all\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"HF_TOKEN\"] = \"<your-token>\"\n",
    "# os.environ[\"WANDB_RESUME\"] = \"must\"\n",
    "# os.environ[\"WANDB_RUN_ID\"] = \"h4h4s471\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a21a89ca-9541-4463-bcd6-3a175474e41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neos/.local/lib/python3.10/site-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/neos/.local/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_model_parameters:18366555\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gpus_list = [0]\n",
    "model_dir = \"sr/TTST/saved_models/ttst_4x.pth\"\n",
    "model = TTST()\n",
    "model = torch.nn.DataParallel(model, device_ids=gpus_list)\n",
    "model = model.cuda()\n",
    "model.load_state_dict(torch.load(model_dir))\n",
    "\n",
    "modelwr = ModelWrapper(model).cuda()\n",
    "\n",
    "print(f\"num_model_parameters:{sum(par.numel() for par in modelwr.parameters())}\")\n",
    "\n",
    "output_dir = \"outputs_sr/TTST\"\n",
    "    \n",
    "# Настраиваем Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(output_dir, \"checkpoints\"),\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=1000,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,  # При наличии поддерживаемого GPU\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=modelwr,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=valid_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Запуск обучения\n",
    "# trainer.train()\n",
    "    \n",
    "    # # Сохраняем итоговую модель\n",
    "    # trainer.save_model(\"./models/swinir_sr_x4_final\")\n",
    "    # print(\"Модель успешно обучена и сохранена!\")\n",
    "    \n",
    "    # # Пример инференса на каком-нибудь LR-изображении\n",
    "    # # Для Swin2SR можно использовать Swin2SRImageProcessor,\n",
    "    # # но ниже пример универсальный с использованием уже \n",
    "    # # готового processor из super_image.\n",
    "    # print(\"Делаем пример инференса на тестовом изображении...\")\n",
    "    \n",
    "    # image_processor = Swin2SRImageProcessor(scale=4)\n",
    "    \n",
    "    # url = \"https://huggingface.co/datasets/super_image/test_images/resolve/main/butterfly.png\"\n",
    "    # response = requests.get(url)\n",
    "    # lr_image_pil = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    \n",
    "    # # Преобразуем LR-картинку в модельный формат\n",
    "    # input_tensor = image_processor(lr_image_pil, return_tensors=\"pt\").pixel_values\n",
    "    \n",
    "    # # Получаем супер-разрешённую картинку\n",
    "    # with torch.no_grad():\n",
    "    #     preds = model.generate(input_tensor)\n",
    "    \n",
    "    # sr_images = image_processor.postprocess(preds)  # Список PIL.Image\n",
    "    # sr_image = sr_images[0]\n",
    "    # sr_image.save(\"sr_result.png\")\n",
    "    \n",
    "    # print(\"Супер-разрешённое изображение сохранено как sr_result.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61c7275d-2f85-4efe-8d51-245d4f2ee31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds.reset_format()\n",
    "train_ds.set_transform(val_transforms)\n",
    "eval_dataset = train_ds\n",
    "\n",
    "eval_folder = \"outputs_sr/data_overlapped_sr_TTST/train_prep\"\n",
    "pred_res = trainer.predict(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be4a7747-9590-4bae-90a8-94c83aa2cd08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8658, 3, 256, 256)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_res.predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd434999-7ee7-444b-849f-3f0c1d267c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8658it [05:40, 25.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved evaluation predictions to: outputs_sr/data_overlapped_sr_TTST/train_prep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred_labels = pred_res.predictions  # [N, H, W]\n",
    "# upvalue\n",
    "pred_labels = (pred_labels * 255).astype(np.uint8)\n",
    "# Создаём поддиректорию в зависимости от шага\n",
    "# eval_folder = os.path.join(output_dir, \"train_predict\")\n",
    "os.makedirs(eval_folder, exist_ok=True)\n",
    "\n",
    "# Сохраняем каждую маску как png\n",
    "try:\n",
    "    eval_dataset.reset_format()\n",
    "    for i, label_map in tqdm(enumerate(pred_labels)):\n",
    "        # img = np.array(eval_dataset[i][\"lr_image\"])\n",
    "        # torch_img = torch.from_numpy(img.astype(float)).moveaxis((0, 1, 2), (1, 2, 0)).unsqueeze(0)\n",
    "        # aug_img = (F.interpolate(torch_img, size=(img.shape[0] * rescale, img.shape[1] * rescale), mode=\"bilinear\", align_corners=False)).numpy().astype(np.uint8).squeeze(0)\n",
    "        # aug_img = np.moveaxis(aug_img, (0, 1, 2), (2, 0, 1))\n",
    "        label_img = Image.fromarray(np.moveaxis(label_map, (0, 1, 2), (2, 0, 1)))\n",
    "        filename = eval_dataset[i]['filename']\n",
    "        img_name = filename.rsplit(\"_\", 2)[0]\n",
    "        label_img.save(os.path.join(eval_folder, img_name, \"img\", filename))\n",
    "except RuntimeError as e:\n",
    "    eval_dataset.set_transform(val_transforms)\n",
    "    raise e\n",
    "eval_dataset.set_transform(val_transforms)\n",
    "\n",
    "print(f\"Saved evaluation predictions to: {eval_folder}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
