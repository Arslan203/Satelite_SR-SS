{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bbcc7b7-63f1-4893-934c-9862cf2f2b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 2.0.5 (you have 1.4.7). Upgrade using: pip install --upgrade albumentations\n",
      "/home/neos/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:datasets:PyTorch version 2.6.0+cu124 available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, AutoImageProcessor\n",
    "from datasets import load_dataset\n",
    "\n",
    "rescale = 4\n",
    "PATCH_SIZE = 256\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "CUSTOM_MEAN = [0.1778, 0.2696, 0.1686]\n",
    "CUSTOM_STD = [0.0942, 0.0915, 0.0762]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65adc14f-d9d7-442f-8efb-2f322a4dfdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 8658 examples [00:01, 7152.40 examples/s]\n",
      "Generating validation split: 918 examples [00:00, 8015.26 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"my_superres_dataset.py\", # –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –∫–ª–∞—Å—Å–æ–º –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "    data_dir=\"./data_overlapped\", # –∫–æ—Ä–µ–Ω—å, –≥–¥–µ –ª–µ–∂–∞—Ç –ø–∞–ø–∫–∏ data/ –∏ data_x4/\n",
    "    # split=[\"train\", \"validation\"], # –∫–∞–∫–∏–µ —Å–ø–ª–∏—Ç—ã –≥—Ä—É–∑–∏—Ç—å\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "train_ds = dataset[\"train\"]\n",
    "valid_ds = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83b88fa8-e2a1-49c9-9dd9-5799bb7a1dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['hr_image', 'lr_image', 'filename'],\n",
       "    num_rows: 8658\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a9bf592-3180-4301-baa9-2a4bac73bbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (PSNR, MSE –∏ –ø—Ä.).\n",
    "    eval_pred ‚Äì —ç—Ç–æ (logits, labels),\n",
    "    –≥–¥–µ logits –∏ labels ‚Äì —Ç–µ–Ω–∑–æ—Ä—ã [batch_size, C, H, W].\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    mse = ((logits - labels) ** 2).mean()\n",
    "    psnr = 10 * math.log10(1 / mse.item()) # –ø—Ä–∏ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ [0,1]\n",
    "    return {\n",
    "    \"mse\": mse.item(),\n",
    "    \"psnr\": psnr,\n",
    "    }\n",
    "\n",
    "# def data_collator(features):\n",
    "#     \"\"\"\n",
    "#     –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –±–∞—Ç—á–∞.\n",
    "#     –ü—Ä–∏–≤–æ–¥–∏—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫ –Ω—É–∂–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É –¥–ª—è Trainer.\n",
    "#     \"\"\"\n",
    "#     lr_batch = []\n",
    "#     hr_batch = []\n",
    "#     for f in features:\n",
    "#         lr_batch.append(f[\"lr\"])\n",
    "#         hr_batch.append(f[\"hr\"])\n",
    "\n",
    "#     lr_batch = torch.stack(lr_batch)\n",
    "#     hr_batch = torch.stack(hr_batch)\n",
    "    \n",
    "#     return {\n",
    "#         \"pixel_values\": lr_batch.float(),  # –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (x_LR)\n",
    "#         \"labels\": hr_batch.float(),        # –º–µ—Ç–∫–∞ (x_HR)\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e76a7bd3-6266-4eac-860d-8281ab1d3423",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "def transforms(examples):\n",
    "    transformA = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        \n",
    "        # –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∏ –¥—Ä—É–≥–∏–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏, \n",
    "        # –Ω–∞–ø—Ä–∏–º–µ—Ä, —è—Ä–∫–æ—Å—Ç—å/–∫–æ–Ω—Ç—Ä–∞—Å—Ç, —à—É–º –∏ —Ç.–¥.\n",
    "        \n",
    "        # –ü—Ä–æ—Å—Ç–µ–π—à–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ\n",
    "        # A.Normalize(mean=CUSTOM_MEAN, std=CUSTOM_STD),  \n",
    "        ToTensorV2()\n",
    "    ],\n",
    "    additional_targets={\n",
    "    \"lr_image\": \"image\"  # –ì–æ–≤–æ—Ä–∏–º, —á—Ç–æ \"lr_image\" ‚Äî —ç—Ç–æ —Ç–æ–∂–µ \"image\"\n",
    "    },\n",
    "    is_check_shapes=False,\n",
    "    )\n",
    "    transformed_images, transformed_labels = [], []\n",
    "    for image, lr_image in zip(examples[\"hr_image\"], examples[\"lr_image\"]):\n",
    "        image, lr_image = np.array(image), np.array(lr_image)\n",
    "        transformed = transformA(image=image, lr_image=lr_image)\n",
    "        transformed_labels.append(transformed[\"image\"])\n",
    "        transformed_images.append(transformed[\"lr_image\"])\n",
    "    examples_res = dict()\n",
    "    examples_res[\"pixel_values\"] = transformed_images\n",
    "    examples_res[\"labels\"] = transformed_labels\n",
    "    return examples_res\n",
    "\n",
    "def transforms_val(examples):\n",
    "    transformA = A.Compose([\n",
    "        # A.HorizontalFlip(p=0.5),\n",
    "        # A.VerticalFlip(p=0.5),\n",
    "        # A.RandomRotate90(p=0.5),\n",
    "        \n",
    "        # –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∏ –¥—Ä—É–≥–∏–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏, \n",
    "        # –Ω–∞–ø—Ä–∏–º–µ—Ä, —è—Ä–∫–æ—Å—Ç—å/–∫–æ–Ω—Ç—Ä–∞—Å—Ç, —à—É–º –∏ —Ç.–¥.\n",
    "        \n",
    "        # –ü—Ä–æ—Å—Ç–µ–π—à–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ\n",
    "        # A.Normalize(mean=CUSTOM_MEAN, std=CUSTOM_STD),  \n",
    "        A.Normalize(mean=[0., 0., 0.], std=[1., 1., 1.]), \n",
    "        ToTensorV2()\n",
    "    ],\n",
    "    additional_targets={\n",
    "    \"lr_image\": \"image\"  # –ì–æ–≤–æ—Ä–∏–º, —á—Ç–æ \"lr_image\" ‚Äî —ç—Ç–æ —Ç–æ–∂–µ \"image\"\n",
    "    },\n",
    "    is_check_shapes=False,\n",
    "    )\n",
    "    transformed_images, transformed_labels = [], []\n",
    "    for image, lr_image in zip(examples[\"hr_image\"], examples[\"lr_image\"]):\n",
    "        image, lr_image = np.array(image), np.array(lr_image)\n",
    "        transformed = transformA(image=image, lr_image=lr_image)\n",
    "        transformed_labels.append(transformed[\"image\"])\n",
    "        transformed_images.append(transformed[\"lr_image\"])\n",
    "    examples_res = dict()\n",
    "    examples_res[\"pixel_values\"] = transformed_images\n",
    "    examples_res[\"labels\"] = transformed_labels\n",
    "    return examples_res\n",
    "\n",
    "from torchvision.transforms import ColorJitter\n",
    "\n",
    "jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"caidas/swin2SR-classical-sr-x4-64\", image_mean=CUSTOM_MEAN, image_std=CUSTOM_STD)\n",
    "\n",
    "def train_transforms(example_batch):\n",
    "    example_batch = transforms(example_batch)\n",
    "    # images = [jitter(x) for x in example_batch[\"lr_image\"]]\n",
    "    images = [x for x in example_batch[\"lr_image\"]]\n",
    "    labels = [x for x in example_batch[\"hr_image\"]]\n",
    "    # inputs = image_processor([images, labels], return_tensors=\"pt\")\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def val_transforms(example_batch):\n",
    "    example_batch = transforms_val(example_batch)\n",
    "    # images = [x for x in example_batch[\"lr_image\"]]\n",
    "    # labels = [x for x in example_batch[\"hr_image\"]]\n",
    "    # print(len(images))\n",
    "    # print(example_batch[\"hr_image\"])\n",
    "    # inputs = image_processor(images + labels, return_tensors=\"pt\")\n",
    "    # example_batch.pop(\"labels\")\n",
    "    return example_batch\n",
    "\n",
    "\n",
    "train_ds.set_transform(train_transforms)\n",
    "valid_ds.set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70b7e338-3085-4be9-b944-d2d07581a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sr.TTST.model_archs.TTST_arc import TTST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "840745ae-1444-4c10-a9af-2a2a2ae333ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import ImageSuperResolutionOutput\n",
    "from typing import Optional, Union, Tuple\n",
    "\n",
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model, patch_size=PATCH_SIZE, accepts_loss_kwargs=False):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.accepts_loss_kwargs = accepts_loss_kwargs\n",
    "        self.patch_size = patch_size\n",
    "        self.loss = torch.nn.L1Loss()\n",
    "\n",
    "    # def forward(self, *args, **kwargs):\n",
    "    #     outputs = self.main(*args, **kwargs)\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: torch.FloatTensor,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, ImageSuperResolutionOutput]:\n",
    "        outputs = self.model(pixel_values)\n",
    "        loss, logits_val = self.loss(outputs, labels), outputs\n",
    "        return ImageSuperResolutionOutput(loss=loss, reconstruction=logits_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcfd46d-e584-4020-b233-5a2774ef7c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
    "os.environ[\"WANDB_WATCH\"] = \"all\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"HF_TOKEN\"] = \"<your-token>\"\n",
    "# os.environ[\"WANDB_RESUME\"] = \"must\"\n",
    "# os.environ[\"WANDB_RUN_ID\"] = \"h4h4s471\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a21a89ca-9541-4463-bcd6-3a175474e41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neos/.local/lib/python3.10/site-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/neos/.local/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_model_parameters:18366555\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gpus_list = [0]\n",
    "model_dir = \"sr/TTST/saved_models/ttst_4x.pth\"\n",
    "model = TTST()\n",
    "model = torch.nn.DataParallel(model, device_ids=gpus_list)\n",
    "model = model.cuda()\n",
    "model.load_state_dict(torch.load(model_dir))\n",
    "\n",
    "modelwr = ModelWrapper(model).cuda()\n",
    "\n",
    "print(f\"num_model_parameters:{sum(par.numel() for par in modelwr.parameters())}\")\n",
    "\n",
    "output_dir = \"outputs_sr/TTST\"\n",
    "    \n",
    "# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(output_dir, \"checkpoints\"),\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=1000,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,  # –ü—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ–≥–æ GPU\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=modelwr,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=valid_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è\n",
    "# trainer.train()\n",
    "    \n",
    "    # # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥–æ–≤—É—é –º–æ–¥–µ–ª—å\n",
    "    # trainer.save_model(\"./models/swinir_sr_x4_final\")\n",
    "    # print(\"–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!\")\n",
    "    \n",
    "    # # –ü—Ä–∏–º–µ—Ä –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –Ω–∞ –∫–∞–∫–æ–º-–Ω–∏–±—É–¥—å LR-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏\n",
    "    # # –î–ª—è Swin2SR –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Swin2SRImageProcessor,\n",
    "    # # –Ω–æ –Ω–∏–∂–µ –ø—Ä–∏–º–µ—Ä —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —É–∂–µ \n",
    "    # # –≥–æ—Ç–æ–≤–æ–≥–æ processor –∏–∑ super_image.\n",
    "    # print(\"–î–µ–ª–∞–µ–º –ø—Ä–∏–º–µ—Ä –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏...\")\n",
    "    \n",
    "    # image_processor = Swin2SRImageProcessor(scale=4)\n",
    "    \n",
    "    # url = \"https://huggingface.co/datasets/super_image/test_images/resolve/main/butterfly.png\"\n",
    "    # response = requests.get(url)\n",
    "    # lr_image_pil = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    \n",
    "    # # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º LR-–∫–∞—Ä—Ç–∏–Ω–∫—É –≤ –º–æ–¥–µ–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç\n",
    "    # input_tensor = image_processor(lr_image_pil, return_tensors=\"pt\").pixel_values\n",
    "    \n",
    "    # # –ü–æ–ª—É—á–∞–µ–º —Å—É–ø–µ—Ä-—Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—É—é –∫–∞—Ä—Ç–∏–Ω–∫—É\n",
    "    # with torch.no_grad():\n",
    "    #     preds = model.generate(input_tensor)\n",
    "    \n",
    "    # sr_images = image_processor.postprocess(preds)  # –°–ø–∏—Å–æ–∫ PIL.Image\n",
    "    # sr_image = sr_images[0]\n",
    "    # sr_image.save(\"sr_result.png\")\n",
    "    \n",
    "    # print(\"–°—É–ø–µ—Ä-—Ä–∞–∑—Ä–µ—à—ë–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∫–∞–∫ sr_result.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61c7275d-2f85-4efe-8d51-245d4f2ee31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds.reset_format()\n",
    "train_ds.set_transform(val_transforms)\n",
    "eval_dataset = train_ds\n",
    "\n",
    "eval_folder = \"outputs_sr/data_overlapped_sr_TTST/train_prep\"\n",
    "pred_res = trainer.predict(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be4a7747-9590-4bae-90a8-94c83aa2cd08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8658, 3, 256, 256)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_res.predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd434999-7ee7-444b-849f-3f0c1d267c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8658it [05:40, 25.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved evaluation predictions to: outputs_sr/data_overlapped_sr_TTST/train_prep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred_labels = pred_res.predictions  # [N, H, W]\n",
    "# upvalue\n",
    "pred_labels = (pred_labels * 255).astype(np.uint8)\n",
    "# –°–æ–∑–¥–∞—ë–º –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —à–∞–≥–∞\n",
    "# eval_folder = os.path.join(output_dir, \"train_predict\")\n",
    "os.makedirs(eval_folder, exist_ok=True)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—É—é –º–∞—Å–∫—É –∫–∞–∫ png\n",
    "try:\n",
    "    eval_dataset.reset_format()\n",
    "    for i, label_map in tqdm(enumerate(pred_labels)):\n",
    "        # img = np.array(eval_dataset[i][\"lr_image\"])\n",
    "        # torch_img = torch.from_numpy(img.astype(float)).moveaxis((0, 1, 2), (1, 2, 0)).unsqueeze(0)\n",
    "        # aug_img = (F.interpolate(torch_img, size=(img.shape[0] * rescale, img.shape[1] * rescale), mode=\"bilinear\", align_corners=False)).numpy().astype(np.uint8).squeeze(0)\n",
    "        # aug_img = np.moveaxis(aug_img, (0, 1, 2), (2, 0, 1))\n",
    "        label_img = Image.fromarray(np.moveaxis(label_map, (0, 1, 2), (2, 0, 1)))\n",
    "        filename = eval_dataset[i]['filename']\n",
    "        img_name = filename.rsplit(\"_\", 2)[0]\n",
    "        label_img.save(os.path.join(eval_folder, img_name, \"img\", filename))\n",
    "except RuntimeError as e:\n",
    "    eval_dataset.set_transform(val_transforms)\n",
    "    raise e\n",
    "eval_dataset.set_transform(val_transforms)\n",
    "\n",
    "print(f\"Saved evaluation predictions to: {eval_folder}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
